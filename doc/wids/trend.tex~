\documentclass[10pt]{article}

\title{A Nonparametric Method for Early Detection of Trending
  Topics} \author{ Stanislav Nikolov$^{*,\dagger}$ and Devavrat Shah$^{\dagger}$\\
  \normalsize \{snikolov,devavrat\}@mit.edu\\
  \normalsize $^{\dagger}$Department of EECS, Massachusetts Institute of Technology\\
 \normalsize $^{*}$Twitter, Inc.}

\usepackage[margin=0.6in]{geometry}
\usepackage{graphicx}
\usepackage{url}
\usepackage{amsmath,amssymb}
\usepackage{comment}
\newcommand{\pr}{\mathbb{P}}
\newcommand{\mb}{\mathbf}

\date{}

\begin{document}

\maketitle

\small

\begin{abstract}
Online social networks can be used as networks of human sensors to detect
important events \cite{Zhao} --- from a global breaking news story to an
incident down the street. It is important to be able to detect such events as
early as possible. To do so, we propose a nonparametric method that predicts
{\em trending topics} on Twitter by comparing a recent activity signal for a
topic to a large collection of historical activity signals for trending and
non-trending topics. We posit that the signals observed for each class of topics
were generated by an unknown set of {\em latent source} signals for that class
according to a stochastic model depending on the {\em distance} between the
observation and its latent source, and propose a class estimator based on this
model. Using our method, we are able to detect trending topics in advance of
Twitter 79\% of the time, with a mean early advantage of 1 hour and 26 minutes,
while maintaining a true positive rate of 95\% and a false positive rate of
4\%. In addition, our method allows for tradeoffs between TPR, FPR, and relative
detection time, scales to large amounts of data, and provides a broadly
applicable framework for nonparametric classification.
\end{abstract}

\subsubsection*{Empirical Observations}
% Notes: 
% * Twitter would like ``Tweet'' to be capitalized.
% * I am only allowed to reproduce the text of my own Tweets.  

%{\bf Empirical Observations}:
On Twitter, users can post short, public messages known as {\em Tweets}. There
are over 400 million Tweets written every day, many of which can be considered
{\em about} one or more topics. For example, this tweet by one of the authors
(Twitter handle @snikolov) {\em ``Stuyvesant High School Taps `Stuy Mafia' at
  Google, Foursquare to Enhance Computer Science Program via @Betabeat
  http://betabeat.com...''} is about ``Stuyvesant High School'', ``Computer
Science'', and so on. Because of the public nature of Twitter, topics can spread
and gain popularity. Topics that gain sudden widespread popularity start {\em
  trending} i.e. they are featured on a list of top ten {\em trending topics} on
Twitter.

Trending topics can typically be detected by a sudden high-magnitude spike in
activity over some baseline of activity \cite{spike1}\cite{spike2}. However, this sudden
spike is often preceded by lower magnitude activity that is indicative of the
topic's imminent popularity. This suggests that we can detect trending topics
earlier by observing this early activity. Thus, we propose to predict whether a
topic will become {\em trending} by comparing recent time series of activity for
the topic to historical time series of activity leading up to other topics
becoming trending, and historical time series of activity from topics that did
not become trending.

We define the {\em activity signal} for a topic in terms of the rate $\rho[n]$
of Tweets about that topic over time, at time bins $n = 1, \dots, N_{obs}$. We
observe that activity is typically characterized by spikes above a baseline
rate, so we further transform the rate to normalize away the baseline
($\rho_b[n] = (\rho[n]/b)^{\beta}$, $b = \sum_n \rho[n] / N_{obs}$) and emphasize spikes
($\rho_{b,s}[n] = |\rho_b[n] - \rho_b[n-1]|^{\alpha}$), according to parameters
$\alpha \geq 1, \beta \geq 1$ (we used $\alpha = 1.2$, $\beta = 1$). In
addition, we convolve the result with a smoothing window to eliminate noise and
effectively measure the volume of Tweets in a sliding window ($\rho_{b,s,c}[n] =
\sum_{m = n - N_{smooth} + 1}^n \rho_{b,s}[m]$). Finally, because the spread of
topics can reasonably be thought of as a branching process, and branching
processes exhibit exponential growth, we measure the volume $\rho_{b,s,c}$ at a
logarithmic scale ($\rho_{b,s,c,l}[n] = \log \rho_{b,s,c}[n]$).

\subsubsection*{Data Model}
Activity signals, even within a single class, are incredibly diverse. Rather
than training a model to distinguish between the activity signals of trending
and non-trending topics, we assume no model structure at all and instead propose
the following nonparametric model relating observed activity signals ({\em
  observations}) to their class labels. Suppose there are two classes: $+$ and
$-$. We posit that there are a number of distinct {\em latent source} signals in
each class that account for all observations in that class. Let us call them
$\mb t_1, \dots, \mb t_n$ for $+$ and $\mb q_1, \dots, \mb q_{\ell}$ for
$-$. Each observation labeled $+$ is assumed to be a noisy version of one of the
latent sources $\mb t_1, \dots, \mb t_n$. Similarly, each observation labeled
$-$ is assumed to be a noisy version of one of the latent sources $\mb q_1,
\dots \mb q_{\ell}$. We do not know what the latent source signals are or even
how many there are. We only know a stochastic model that relates an observation
to its latent source.

Let the observation $\mb s$ be the most recent $N_{obs}$ samples of an infinite
stream $\mb s_{\infty}$ of activity. An observation $\mb s$ is {\em generated}
by a latent source signal $\mb q$ according to the stochastic model
\begin{gather}
\pr(\mb s \text{ generated by } \mb q) \propto \exp\left(-\gamma d(\mb s,\mb q)
\right)
\end{gather}
where $d$ is a symmetric, positive definite, and convex distance function (we
used the Euclidean norm). To determine whether the observation belongs to $+$ or
$-$, we use of a set of example, or {\em reference} activity signals
$\mathcal{R}_+$ from $+$ and $\mathcal{R}_-$ from $-$. Under our model the
observation must belong to $+$ if it has the same latent source as one of the
reference signals in $\mathcal{R}_+$. Similarly, the observation must belong to
$-$ if it has the same latent source as one of the reference signals in
$\mathcal{R}_-$. Hence, the probability that the observation belongs to $+$ is
\begin{align}
\pr(+ \;|\; \mb s) &= \sum_{\mb r \in \mathcal{R}_+} \pr(\mb s \text{ belongs to } +, \mb s \text{ shares a latent source
  with } \mb r) = \sum_{\mb r \in \mathcal{R}_+} \sum_{j = 1}^n \pr( \mb s \text{ generated by }
\mb t_j, \mb r \text{ generated by } \mb t_j)\notag\\
&\propto \sum_{\mb r \in \mathcal{R}_+} \sum_{j = 1}^n \exp{\left(-\gamma\left(
    d(\mb s,\mb t_j) + d(\mb r,\mb t_j) \right)\right)} \approx \sum_{\mb r \in \mathcal{R}_+} \exp{\left(-\gamma \min_{j} \left( d(\mb s,\mb t_j) + d(\mb r,\mb t_j) \right) \right)} \approx \sum_{\mb r \in \mathcal{R}_+} \exp{\left(-C\gamma d(\mb s,\mb r)\right)}.
\end{align}
The second to last approximation relies on the fact that for large enough
$\gamma$, the term with the smallest exponent will dominate the sum over the
latent sources. For the last approximation, we observe that the global minimum
of $d(\mb s,\mb t) + d(\mb r,\mb t)$ over all signals $\mb t$ is $Cd(\mb r, \mb
s)$ for some $C>0$ and is achieved at $\mb t^* = (\mb s + \mb r)/2$. The
approximation is valid when the minimizing latent source $\mb t_{j^*}$ is
sufficiently close to the global minimizer $\mb t^*$. We approximate $\pr(-
\;|\; \mb s)$ in a similar fashion. In essence, because the latent sources are
unknown, we cannot directly compare the observation with them. Instead we
compare the observation to the reference signals, using them as a proxy for the
latent sources. Figure \ref{fig:method} illustrates this.
\begin{figure}
\begin{center}
%\includegraphics[width=7.5in]{allfig.png}
\includegraphics[height=0.75in]{latent_source_color}
\end{center}
\caption{\label{fig:method}{\bf Left}: Reference signals from each class. {\bf
    Right}: Finding the latent source signal $\mb t_{j^*}$ that minimizes $d(\mb
  s, \mb t_j) + d(\mb r, \mb t_j)$, i.e. the latent source signal most likely to
  have generated both $\mb s$ and $\mb r$.}
\end{figure}
We classify the observation according to $f(\mb s) = \mathrm{sgn}(R(\mb s)
- \theta)$, where $R(\mb s) = \pr(+ | \mb s)/\pr(- | \mb s)$ and $\theta$ is a
threshold. This approach has an appealing interpretation: to classify an
observation, one simply computes the distance from the observation to all
examples from each class. This can be done in parallel on enormous data sets.

\subsubsection*{Results and Conclusion}
We collected 500 trending and 500 non-trending topics during the month of June,
2012 and 10\% of the Tweets containing those topics. We used a 50/50 split
between reference signals and observations and performed detection in a window
of size $2h_{ref}$ hours, centered around the true onset of the trending topic
if the topic was trending or chosen randomly otherwise. Using this small sample
of Tweets, our method is capable of detecting trends before Twitter does 79\% of
the time, with a mean early advantage of 1 hour and 26 minutes, while
maintaining a 95\% true positive rate and a 4\% false positive rate (Figure
\ref{fig:results} (left)). Figure \ref{fig:results} (right) shows an example
early detection in which our method detected the trending topic ``Miss Rhode
Island'' over 2 hours before Twitter did. In Figure \ref{fig:results} (center)
the envelope of ROC curves for all combinations of parameters shows the ability of
our method to perform well under a variety of tradeoffs between types of error.
In addition, our method servers as a broadly applicable framework for scalable
nonparametric classification in the presense of large amounts of data.
\begin{figure}
\begin{center}
%\includegraphics[width=7.5in]{allfig.png}
\includegraphics[height=1.0in]{best}\includegraphics[height=1.30in]{roc_env}\includegraphics[height=1.30in]{example}
\end{center}
\caption{\label{fig:results}. Our method is capable of early detection of trending topics while maintaining a low rate of error and provides the flexibility for tradeoffs between error types.}
\end{figure} 

\begin{footnotesize}
\bibliographystyle{acm}
\bibliography{trend}
\end{footnotesize}
\end{document}

\begin{comment}
We observe that trending topics are distinguished from nontrending topics by
their pattern of activity leading up to the time the topic is detected by
Twitter (the true onset)

We define the activity signal for a topic in terms of the rate of Tweets about
that topic. To define the activity signal for a topic, let $\rho[n]$ be the
number of Tweets in the $n$th time bin, for $n=1,\dots,N$. Since this is the
discrete derivative of the volume of Tweets over time, we shall call this the
{\em rate} of Tweets. To remove the effect of different baseline rates, we
construct a baseline-normalized rate $\rho_b[n] = \left(\rho[n]/b
\right)^{\beta}$ using the baseline $b = (\sum_{n} \rho[n])/N$. To emphasize
spikes we define $\rho_{b,s}[n] = \big|\rho_b[n] - \rho_b[n-1]\big|^{\alpha}$
{\bf Spike, Log, Smoothing}

We observe that trending topics are distinguished from nontrending
topics by their pattern of activity leading up to the time the topic is detected
by Twitter (the true onset). In particular, the activity of a soon to be
trending topic is characterized by frequent sharp jumps of high magnitude over
some baseline activity. Let $\rho[n]$ be the discrete derivative at time step
$n$ of the volume $v(t)$ of Tweets about a topic over time, for $n = 1, \dots,
N$. We shall call this the rate of Tweets. 
\end{comment}
