%% The text of your abstract and nothing else (other than comments) goes here.
%% It will be single-spaced and the rest of the text that is supposed to go on
%% the abstract page will be generated by the abstractpage environment.  This
%% file should be \input (not \include 'd) from cover.tex.

In supervised classification, one attempts to learn a model of how objects map
to labels. This involves selecting the best model from some model space,
prefering a model that fits the data but has low complexity. The choice of model
space encodes assumptions about the problem, for example, via a kernel and its
associated Reproducing Kernel Hilbert Space. We propose a different setting for
model specification and selection in supervised learning based on a {\em latent
  source model}. In this setting, the model is specified by a small collection
of unknown {\em latent sources}. We posit that the data were generated by these
latent sources and that there is a stochastic model relating latent sources and
observations.

With this setting in mind, we propose a classification method that avoids
searching over the model space, and is in fact, entirely unaware of what the
latent sources are or how many there are. Instead, our method relies on large
amounts of data as a proxy for the unknown latent sources. We perform
classification by directly computing the conditional class probabilities for an
observation based on our stochastic model. This approach results in a surprising
and natural interpretation --- that to see how likely it is that an observation
belongs to a certain class, we can simply observe how much it resembles other
examples of that class. This is well-suited to problems with large amounts of
labeled data.

We extend this approach to the problem of online timeseries classification. In
the binary case, we derive a maximum likelihood (TODO: how is it ML?) estimator
for online signal detection and an associated implementation that is simple,
efficient, and scalable. We demonstrate the merit of our approach by applying it
to the task of detecting {\em trending topics} on Twitter, and show that in many
cases, we can detect trending topics before they are identified by Twitter while
maintaining a low rate of error.

%For example, in Tikhonov Regularization and associated special cases such as
%SVMs or Regularized Least Squares, the model space is specified by the choice of
%a kernel, which encodes similarities between data points, and a corresponding
%Reproducing Kernel Hilbert Space from which the classification function is
%chosen.

%We propose that belonging to a particular class amounts to having been generated
%by the same source as some labeled example belonging to that class. 
