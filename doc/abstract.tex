%% The text of your abstract and nothing else (other than comments) goes here.
%% It will be single-spaced and the rest of the text that is supposed to go on
%% the abstract page will be generated by the abstractpage environment.  This
%% file should be \input (not \include 'd) from cover.tex.

In supervised classification, one attempts to learn a model of how objects map
to labels by selecting the best model from some model space. The choice of model
space encodes assumptions about the problem. We propose a setting for model
specification and selection in supervised learning based on a {\em latent source
model}. In this setting, we specify the model by a small collection of unknown
{\em latent sources} and posit that there is a stochastic model relating latent
sources and observations. With this setting in mind, we propose a nonparametric
classification method that is entirely unaware of the structure of these latent
sources. Instead, our method relies on the data as a proxy for the unknown
latent sources. We perform classification by computing the conditional class
probabilities for an observation based on our stochastic model. This approach
has an appealing and natural interpretation --- that an observation belongs
to a certain class if it sufficiently resembles other examples of that class.

We extend this approach to the problem of online time series classification. In
the binary case, we derive an estimator for online signal detection and an
associated implementation that is simple, efficient, and scalable. We
demonstrate the merit of our approach by applying it to the task of detecting
{\em trending topics} on Twitter. Using a small sample of Tweets, our method can
detect trends before Twitter does 79\% of the time, with a mean early advantage
of 1.43 hours, while maintaining a 95\% true positive rate and a 4\% false
positive rate. In addition, our method provides the flexibility to perform well
under a variety of tradeoffs between types of error and relative detection time.

%(TODO: how is it ML?) 

%For example, in Tikhonov Regularization and associated special cases such as
%SVMs or Regularized Least Squares, the model space is specified by the choice of
%a kernel, which encodes similarities between data points, and a corresponding
%Reproducing Kernel Hilbert Space from which the classification function is
%chosen.

%We propose that belonging to a particular class amounts to having been generated
%by the same source as some labeled example belonging to that class. 
