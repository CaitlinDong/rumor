\chapter{Application: Identifying Trending Topics on Twitter}
\label{ch:data}

%TODO:

In this chapter, we consider the application of the method and algorithm proposed
in Chapters \ref{ch:method} and \ref{ch:alg} toward detection of trending topics
on Twitter. We discuss the Twitter service, the collection and pre-processing of
data, and the experimental setup for the detection task.

\section{Overview}
\subsection{Overview of Twitter}
Twitter is a real-time messaging service and information network. Users of
Twitter can post short (up to 140 characters) messages called {\em Tweets},
which are then broadcast to the users' {\em followers}. Users can also engage in
conversation with one another. By default, Tweets are public, which means that
anyone can see them and potentially join a conversation on a variety of topics
being discussed. Inevitably, some topics gain relatively sudden popularity on
Twitter. For example, a popular topic might reflect an external event such as a
breaking news story or an internally generated inside joke or game. Twitter
surfaces such topics in the service as a list of top ten trending topics. 

\subsection{Twitter-Related Definitions}
Talking about Tweets, topics, trends and trending topics can be ambiguous, so
here we make precise our usage of these and related terms.

\begin{defn}[Topic]
  We define a {\bf topic} to be a phrase consisting of one or more {\bf words}
  delimited by spacing or punctuation. A word may be any sequence of characters
  and need not be an actual dictionary word.
\end{defn}

\begin{defn}[Tweet about topic]
A Tweet is {\bf about} a topic if it contains the topic as a substring. Tweets
can be about many topics.
\end{defn}

\begin{example}
The following tweet by the author (handle @snikolov) contains the string
``matlab.'' Hence, it is considered to be {\em about} the topic ``matlab.''
\begin{quote}
``matlab symbolic eigendecomposition. expressions with 25000+ characters are not
always the most interpretable thing.''
\end{quote}
\end{example} 

\begin{defn}[Trending topic]
A {\bf trending topic} is a topic that is currently on the list of trending
topics on Twitter. If a topic was ever a trending topic during a period of time,
we say that the topic {\bf trended} during that time period.
\end{defn}

\begin{defn}[Trend] A trending topic will also occasionally be referred to as a {\bf trend}
  for short.
\end{defn}

\begin{defn}[Trend onset]
  The {\bf trend onset} is the time that a topic first trended during a
  period of time.
\end{defn}

\begin{example}
If the topic ``earthquake'' is currently in the trending topics list on Twitter,
we say that ``earthquake'' is {\em trending}, and that earthquake is a {\em
  trend}. The topic ``earthquake'' has a {\em trend onset}, which is the first
time it was trending in a given period of time. This could, for example,
correspond to when the earthquake happened. After ``earthquake'' is no longer
trending, we say that ``earthquake'' {\em trended}.
\end{example}

\subsection{Problem Statement}
At any given time there are many topics being talked about on Twitter. Of these,
some will trend at some point in the future and others will not. We wish to
predict which topics will trend. The earlier we can predict that a topic will
trend, the better. Ideally, we would like to do this while maintaining a low
rate of error (false detections and false non-detections).

\subsection{Proposed Solution}
Our approach to detecting trending topics is as follows. First, we gather
examples of topics that trended and topics that did not trend during
some period of time. Then, for each topic, we collect Tweets about that topic
and generate a time series of the activity of that topic over time. We then use
those time series as {\em reference signals} (cf. Chapter \ref{ch:method}) and
apply the classification method and algorithm described in Chapters
\ref{ch:method} and \ref{ch:alg}.

\section{Data}
\subsection{Data Collection}
The online time series classification method detailed in Chapters
\ref{ch:method} and \ref{ch:alg} requires a set of reference signals
corresponding to topics that trended and a set of reference signals
corresponding to topics that did not trend during a time window of
interest. These reference signals represent historical data against which we can
compare our most recent observations to do classification.

The data collection process can be summarized as follows. First, we collected
500 examples of topics that trended at least once between June 1, 2012 and June
30, 2012 (hereafter referred to as the {\em sample window}) and 500 examples of
topics that never trended during the sample window. We then sampled Tweets from the
sample window and labeled each Tweet according to the topics mentioned
therein. Finally, we constructed a reference signal for each topic based on the
Tweet activity corresponding to that topic.

We obtained all data directly from Twitter via the MIT VI-A thesis
program. However, the type as well as the amount of data we have used is all
publicly available via the Twitter API.

\subsubsection{Topics}
We collected a list of all trending topics on Twitter from June 1, 2012 to June
30, 2012 (the {\em sample window}) as well as the times that they were trending
and their rank in the trending topics list on Twitter. Of those, we filtered out
topics whose rank was never better than or equal to 3. In addition, we filtered
out topics that did not trend for long enough (the time of the first appearance
to the time of the last appearance is less than 30 minutes) as well as topics
that reappear multiple times during the sample window (the time of the first
appearance to the time of the last appearance is greater than 24 hours). The
former eliminates many topics that are spurious and only trend for a very short
time. The latter eliminates topics that correspond to multiple events. For
example, the name of a football player might trend every time there is an
important game. We would like to avoid such ambiguity and restrict each trending
topic to correspond to a single underlying event within the sample window.

We collected topics that did not trend during the sample window in two
steps. First, we sampled a list of $n$-grams (phrases consisting of $n$
``words'') occurring on Twitter during the sample window for $n$ up to 5. We
filtered out $n$-grams that contain any topic that trended during the sample
window, using the original, unfiltered list of all topics that trended during
the sample window. For example, if ``Whitney Houston'' trended during the sample
window, then ``Whitney'' would be filtered out of the list of topics that
did not trend. We also removed $n$-grams shorter than three characters, as most
of these did not appear to be meaningful topics. Lastly, we sampled 500
$n$-grams uniformly from the filtered list of $n$-grams to produce the final
list.

\subsubsection{Tweets}
% TODO: Is 10% accessible publicly? For a price?
We sampled 10\% of all public Tweets from June 1, 2012 to June 30, 2012
inclusive. We labeled each Tweet with the topic or topics contained therein
using a simple regular expression match between the Tweet text and the topic
text. In addition to the Tweet text, we recorded the date and time the Tweet was
authored.

%TODO: Give the parameters names!!!
\subsection{From Tweets to Signals}
We discuss the process of converting the timestamped Tweets for a given topic
into a reference signal. Each of the steps below is followed in order for each
topic.

\subsubsection{Tweet Rate}
As a first step toward converting timestamped Tweets about a topic into a
signal, we bin the Tweets into time bins of a certain length. We use time bins
of length two minutes. Let $\rho[n]$ be the number of Tweets about a topic in
the $n$th time bin. Let the cumulative volume of of Tweets up to time $n$ be
\begin{gather}
v[n] = \sum_{m \leq n} \rho[m]
\end{gather}
Thus, effectively $\rho[n]$ is the discrete derivative of the continuous
cumulative volume $v(t)$ over time i.e. $\rho(t) = \dot{v}(t)$. Therefore, we
shall call $\rho[n]$ the rate of the signal at time step $n$. Figure
\ref{fig:rate} shows this rate for a topic over the whole sample window.
\begin{figure}[h!]
\begin{center}
\includegraphics[width=6in]{../fig/final/signal_transform/rate.eps}
\end{center}
\caption{\label{fig:rate} The rate of a topic over the sample window.}
\end{figure}

\subsubsection{Baseline Normalization}
A first glance at the data reveals that many non-trending topics have a
relatively high rate and volume of Tweets, and many trending topics have a
relatively low rate and volume of Tweets. One important difference is that many
non-trending topics have a high {\em baseline rate} of activity while most
trending topics are preceded by little, if any, activity prior to gaining sudden
popularity. For example, a non-trending topic such as 'city' is likely to have a
consistent baseline of activity because it is a common word. To emphasize the
parts of the rate signal above the baseline and de-emphasize the parts below the
baseline, we define a baseline $b$ as 
\begin{gather}
b = \sum_{n} \rho[n]
\end{gather}
and a baseline-normalized signal $\rho_{b}$ as
\begin{gather}
\rho_{b}[n] = \left(\frac{\rho[n]}{b}\right)^{\beta}.
\end{gather}
The exponent $\beta$ controls how much we reward and penalize rates above and
below the baseline rate. In this thesis, we use $\beta = 1$. Figure
\ref{fig:baseline_spikes} shows rate signals without any baseline normalization
and their baseline-normalized versions.

\subsubsection{Spike Normalization}
%TODO: This is weak. Do we want to be rewarding big jumps or sudden jumps? Not
%clear which one you're doing here, and not clear which is more justifiable.
Another difference between the rates of Tweets for trending topics and that of
non-trending topics is the number and magnitude of spikes. The Tweet rates for
trending topics typically contains larger and more sudden spikes than that of
non-trending topics. We reward such spikes by emphasizing them, while
de-emphasizing smaller spikes. To do so, we define a
baseline-and-spike-normalized rate
\begin{gather}
\rho_{b,s}[n] = \big|\rho_b[n] - \rho_b[n-1]\big|^{\alpha}
\end{gather}
in terms of the already baseline-normalized rate $\rho_b$.  The parameter
$\alpha > 1$ controls how much spikes are rewarded. We use $\alpha =
1.2$. Figure \ref{fig:baseline_spikes} shows the effect of this spike-based
transformation.

\subsubsection{Smoothing}
Tweet rates, and the aforementioned transformations thereof, tend to be noisy,
especially for small time bins. To mitigate this, we convolve the signal with a
smoothing window of size $N_{smooth}$. Applied to the
spike-and-baseline-normalized signal $\rho_{b,s}$, this yields the convolved
version
\begin{gather}
\rho_{b,s,c}[n] = \sum_{m = n-N_{smooth}+1}^{n} \rho_{b,s}[m].
\end{gather}
Figure \ref{fig:smooth} shows the effect of smoothing with various window sizes.

\subsubsection{Branching Processes and Logarithmic Scale}
It is reasonable to think of the spread of a topic from person to person as a
branching process. A branching process is a model of the growth of a population
over time, in which each individual of a population in a given generation
produces a random number of individuals in the next generation. While we do not
know the details of how a topic spreads, we do know that in a wide generality of
branching processes, the growth of the population is exponential with time, with
the exponent depending on the details of the model \cite{AthreyaNey}. It is
reasonable, then, to measure the volume of tweets at a logarithmic scale to
reveal these details. Asur et al. confirm that the spread of topics on Twitter
can be modeled as a branching process and also propose a logarithmic scaling
\cite{Asur}. Therefore, as a final step, we take the logarithm of the signal
constructed so far to produce the signal
\begin{gather}
\rho_{b,s,c,l}[n] = \log \rho_{b,s,c}[n].
\end{gather}
Figure \ref{fig:log} shows a sample of signals and their log-scaled versions.

\subsubsection{Constructing a Reference Signal}
The signal $\rho_{b,s,c,l}[n]$ resulting from the steps so far is as long as the
entire time window from which all Tweets were sampled. Such a long signal is not
particularly useful as a reference signal. Recall from chapter \ref{ch:alg} that
to see how much the recent trajectory of the observed signal resembles part of a
reference signal, we have to traverse the full length of the reference signal in
order to find the piece that most closely resembles the recent observed
trajectory. If the reference signal for a topic that trended spans too long of a
time window, only a small portion of it will represent activity surrounding the
onset of the trend. In addition, it is inefficient to compare the recently
observed trajectory to a reference signal that is needlessly long. Hence, it is
necessary to select a small slice of signal from the much longer rate signal. In
the case of topics that trended, we select a slice that terminates at the first
onset of trend. That way, we capture the pattern of activity leading up to the
trend onset, which is crucial for recognizing similar pre-onset activity in the
observed signal. We do not include activity after the true onset because once
the topic is listed in the trending topics list on Twitter, we expect the
predominant mode of spreading to change. For topics that did not trend, we
assume that the rate signal is largely stationary and select slices with random
start and end times. For simplicity, all slices are a fixed size of $N_{ref}$.

Figures \ref{fig:baseline_spikes} through \ref{fig:log} show the samples of
reference signals with various combinations of the transformations described in
this section.
\begin{figure}[h!]
\begin{center}
\includegraphics[width=3.10in]{../fig/final/signal_transform/log.eps}\includegraphics[width=3.10in]{../fig/final/signal_transform/log_baseline.eps}
\includegraphics[width=3.10in]{../fig/final/signal_transform/log_baseline_spikes.eps}
\end{center}
\caption{\label{fig:baseline_spikes} Reference signals of either class are hard to tell apart without normalization. {\bf Top left}: no baseline or spike normalization. {\bf Top right}: Baseline normalization. {\bf Bottom}: Baseline and spike-based normalization.}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=3.10in]{../fig/final/signal_transform/smooth_1.eps}\includegraphics[width=3.10in]{../fig/final/signal_transform/smooth_10.eps}
\includegraphics[width=3.10in]{../fig/final/signal_transform/smooth_100.eps}
\end{center}
\caption{\label{fig:smooth} The results of smoothing the reference signals (with spike and baseline normalization previously applied) with windows of size 1 (2 minutes, i.e. no smoothing), 10 (20 minutes), and 100 (3 hours, 20 minutes).}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=3.10in]{../fig/final/signal_transform/no_log.eps}\includegraphics[width=3.1in]{../fig/final/signal_transform/yes_log.eps}
\end{center}
\caption{\label{fig:log} Logarithmically scaled reference signals (with spike and baseline normalization previously applied) allow one to make finer-grained distinctions between signals. {\bf Left}: Not logarithmically scaled. {\bf Right}: Logarithmically scaled.}
\end{figure}

\clearpage

\section{Experiment}
We propose an experiment to measure our algorithm's performance on two fronts:
error rate and relative detection time. We divide the set of topics into a
training set and a test set using a 50/50 split. For each topic in the test
set, we wish to predict if the topic will trend. If the topic really did trend,
we wish to detect it as early as possible relative to the true trend onset while
incurring minimal error.

\subsection{Detection Setup}
In principle, to test the detection algorithm, one would step through the signal
in the entire sample window for each topic in the test set and report the time
of the first detection, or that there were no detections. In practice, we take a
shortcut to avoid looking through the entire signal based on the following
observations about the activity of topics that trended and topics that did
not. First, for topics that trended, there is little, if any activity aside from
that surrounding the true onset of the trend. In the rare event that a detection
is made very far from the true onset, it is reasonable to assume that this
corresponds to a completely different event involving that topic and we can
safely ignore it. Thus, the only part of the signal worth looking at is the
signal within some time window from the true onset of the trend. Second, topics
that did not trend exhibit relatively stationary activity. That is, the signal
usually looks roughly the same over the entire sample window. Therefore, it is
reasonable to perform detection only on a piece of the signal as an
approximation to the true detection performance.

We perform detection over a window of $2N_{obs}$ samples --- twice the length of a
reference signal. For convenience and future use, we define this in terms of hours.

\begin{defn}
Let $h_{ref}$ be the number of hours corresponding to $N_{ref}$ samples. At 2 minutes per sample, $h_{ref}$ is given by $N_{ref} / 30$.
\end{defn}

For test topics that have trended, we do detection on the window spanning
$2h_{ref}$ hours centered at the true trend onset. For topics that did not
trend, we randomly choose a window of the desired size. Note that, although this
seems to require {\em a priori} knowledge of whether the test topic ever trended
or not, this is only a consequence of the shortcut we take to not do detection
over the entire sample window.

\subsection{Parameter Exploration and Trials}
We explore all combinations of the following ranges of parameters, excluding
parameter settings that are incompatible (e.g. $N_{obs} > N_{ref}$). For each
combination, we conducted 5 random trials.

\begin{itemize}
\item $\gamma$: 0.1, 1, 10.
\item $N_{obs}$: 10, 80, 115, 150.
\item $N_{smooth}$: 10, 80, 115, 150.
\item $h_{ref}$: 3, 5, 7, 9.
\item $D_{req}$: 1, 3, 5.
\item $\theta$: 0.65, 1, 3.
\end{itemize}

\subsection{Evaluation}
To evaluate the performance of our method, we compute the false positive rate
and true positive rate for each experiment, averaged over all trials. In the
case of true detections, we compute the detection time relative to the true
onset of the trending topic.

Before presenting the results in the following chapter, we note the following
important difference between the general detection method employed herein and
that of Twitter. Twitter produces a list of top ten trending topics, while we
perform detection based on a score and a threshold, and do not limit the number
of topics detected as trending at any given time. This could cause noticeable
discrepancies between the topics detected. For example, an otherwise popular
emerging topic might not be detected as a trend if there are many other
important topics being discussed at the moment. Despite these differences, we
show in the next chapter that our algorithm can achieve good performance
relative to that of Twitter.
