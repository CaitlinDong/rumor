\chapter{Detection Method}
\label{ch:method}

I propose a signal classification framework in which we assume that a signal of a particular category was generated by one of a set of latent signals representing that category.

Suppose that whenever a particular event happens it generates a signal measuring a particular observable property of the event. If the same type of event were to happen many times, we can suppose that the signals generated are noisy versions of a {\em latent} signal corresponding to that type of event.

Suppose that $\Pi$ is the set of all possible events, $\Pi_+$ the set of all possible viral events and $\Pi_-$ the set of all possible non-viral events. Suppose that all events are disjoint, in the sense that no two events can happen at once. Then if a viral event happens, it must be a noisy version of exactly one of the events in $\Pi_+$.

We observe a signal and would like to determine if this signal was generated by a topic that will become viral. To do this, we look at the signal for each positive event and ask if the test signal was generated by the same type of event that generated the positive event. We will assume that each signal was generated by a seperate event in $\Pi$. 

Let us define the following variables:

\begin{itemize}
\item $s_i$ --- an reference signal.
\item $t_j$ --- a latent signal.
\item $s$ --- the observed signal that we would like to classify.
\item $V$ --- the event that $s$ was generated from a viral event. 
\item $\mathcal{R}_+$ --- the indices of the set of viral reference signals.
\item $\mathcal{R}_-$ --- the indices of the set of nonviral reference signals.
\item $\mathcal{L}_+$ --- the indices of the set of latent signals corresponding to a viral reference signal
\item $\mathcal{L}_-$ --- the indices of the set of latent signals corresponding to a nonviral reference signal.
\end{itemize}

The probability that the obseved signal is viral is therefore
\begin{align}
\pr(V | s) &= \sum_{i \in \mathcal{R}_+}  \pr(V , s \text{ shares a latent event with } s_i | s)\notag\\
&= \sum_{i \in \mathcal{R}_+} \sum_{j \in \Pi_+} \pr( s \text{ generated by } t_j, s_i \text{ generated by } t_j)\notag\\
&= \sum_{i \in \mathcal{R}_+} \sum_{j \in \Pi_+} \exp{\left[-\gamma d(s,t_j)\right]} \exp{\left[-\gamma d(s_i,t_j)\right]}\notag\\
&= \sum_{i \in \mathcal{R}_+} \sum_{j \in \Pi_+} \exp{\left[-\gamma\left( d(s,t_j) + d(s_i,t_j) \right)\right]}
\end{align}
where $d(s,t)$ is the distance between signals $s$ and $t$. For large $\gamma$, the term with the smallest exponent will dominate the sum over $\Pi_+$ and we can write
\begin{align}
\pr(V | s) &\approx \sum_{i \in \mathcal{R}_+} \exp{\left[-\gamma \min_{j \in \Pi_+} \left( d(s,t_j) + d(s_i,t_j) \right) \right]}
\end{align}
We now argue that for reasonable distance metrics, we can write
\begin{gather*}
\min_{j \in \Pi_+} \left( d(s,t_j) + d(s_i,t_j) \right) = Cd(s,s_i)
\end{gather*}
for some $C$.
